{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(__doc__)\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Supervised-Learning-versus-Unsupervised-Learning-Mathworks-nd.png.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./supervised_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: no Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./unsupervised_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1*KrcZK0xYgTa4qFrVr0fO2w.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Clustering**\n",
    "\n",
    "1. To begin, we first select a number of classes/groups to use and randomly initialize their respective center points. To figure out the number of classes to use, it’s good to take a quick look at the data and try to identify any distinct groupings. The center points are vectors of the same length as each data point vector and are the “X’s” in the graphic above.\n",
    "\n",
    "\n",
    "2. Each data point is classified by computing the distance between that point and each group center, and then classifying the point to be in the group whose center is closest to it.\n",
    "\n",
    "\n",
    "3. Based on these classified points, we recompute the group center by taking the mean of all the vectors in the group.\n",
    "\n",
    "\n",
    "4. Repeat these steps for a set number of iterations or until the group centers don’t change much between iterations. You can also opt to randomly initialize the group centers a few times, and then select the run that looks like it provided the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Importing the dataset\n",
    "data = pd.read_csv('./sample_data.csv')\n",
    "data = data.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "print(\"Input Data and Shape\")\n",
    "print(data.shape)\n",
    "data.head()\n",
    "\n",
    "# Getting the values and plotting it\n",
    "f1 = data['V1'].values\n",
    "f2 = data['V2'].values\n",
    "X = np.array(list(zip(f1, f2)))\n",
    "plt.scatter(f1, f2, c='black', s=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance Caculator\n",
    "def dist(a, b, ax=1):\n",
    "    return np.linalg.norm(a - b, axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_point_1 = np.array((1,1))\n",
    "sample_point_2 = np.array((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist(sample_point_1, sample_point_2, ax=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "k = 3\n",
    "# X coordinates of random centroids\n",
    "C_x = np.random.randint(0, np.max(X)-20, size=k)\n",
    "# Y coordinates of random centroids\n",
    "C_y = np.random.randint(0, np.max(X)-20, size=k)\n",
    "C = np.array(list(zip(C_x, C_y)), dtype=np.float32)\n",
    "print(\"Initial Centroids\")\n",
    "print(C)\n",
    "\n",
    "# Plotting along with the Centroids\n",
    "plt.scatter(f1, f2, c='#050505', s=7)\n",
    "plt.scatter(C_x, C_y, marker='*', s=200, c='g')\n",
    "\n",
    "# To store the value of centroids when it updates\n",
    "C_old = np.zeros(C.shape)\n",
    "# Cluster Lables(0, 1, 2)\n",
    "clusters = np.zeros(len(X))\n",
    "# Error func. - Distance between new centroids and old centroids\n",
    "error = dist(C, C_old, None)\n",
    "# Loop will run till the error becomes zero\n",
    "while error != 0:\n",
    "    # Assigning each value to its closest cluster\n",
    "    for i in range(len(X)):\n",
    "        distances = dist(X[i], C)\n",
    "        cluster = np.argmin(distances)\n",
    "        clusters[i] = cluster\n",
    "    # Storing the old centroid values\n",
    "    C_old = deepcopy(C)\n",
    "    # Finding the new centroids by taking the average value\n",
    "    for i in range(k):\n",
    "        points = [X[j] for j in range(len(X)) if clusters[j] == i]\n",
    "        C[i] = np.mean(points, axis=0)\n",
    "        print('centroid')\n",
    "        print(C)\n",
    "        print('')\n",
    "    error = dist(C, C_old, None)\n",
    "\n",
    "colors = ['r', 'g', 'b', 'y', 'c', 'm']\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(k):\n",
    "        points = np.array([X[j] for j in range(len(X)) if clusters[j] == i])\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=7, c=colors[i])\n",
    "ax.scatter(C[:, 0], C[:, 1], marker='*', s=200, c='#050505')\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "==========================================================\n",
    "scikit-learn\n",
    "==========================================================\n",
    "'''\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "# Fitting the input data\n",
    "kmeans = kmeans.fit(X)\n",
    "# Getting the cluster labels\n",
    "labels = kmeans.predict(X)\n",
    "# Centroid values\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Comparing with scikit-learn centroids\n",
    "print(\"Centroid values\")\n",
    "print(\"Scratch\")\n",
    "print(C) # From Scratch\n",
    "print(\"sklearn\")\n",
    "print(centroids) # From sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advantages: fast, linear complexity O(n).\n",
    "\n",
    "disadvantages: \n",
    "1. You have to select how many groups/classes exist. \n",
    "2. K-means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs.\n",
    "\n",
    "K-Medians \n",
    "1. less sensitive to outliers \n",
    "2. much slower for larger datasets as sorting is required on each iteration. O(nlogn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Shift Clustering\n",
    "\n",
    "Mean shift clustering is a sliding-window-based algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./1*bkFlVrrm4HACGfUzeBnErw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To explain mean-shift we will consider a set of points in two-dimensional space like the above illustration. We begin with a circular sliding window centered at a point C (randomly selected) and having radius r as the kernel. Mean shift is a hill climbing algorithm which involves shifting this kernel iteratively to a higher density region on each step until convergence.\n",
    "\n",
    "2. At every iteration the sliding window is shifted towards regions of higher density by shifting the center point to the mean of the points within the window (hence the name). The density within the sliding window is proportional to the number of points inside it. Naturally, by shifting to the mean of the points in the window it will gradually move towards areas of higher point density.\n",
    "\n",
    "3. We continue shifting the sliding window according to the mean until there is no direction at which a shift can accommodate more points inside the kernel. Check out the graphic above; we keep moving the circle until we no longer are increasing the density (i.e number of points in the window).\n",
    "\n",
    "4. This process of steps 1 to 3 is done with many sliding windows until all points lie within a window. When multiple sliding windows overlap the window containing the most points is preserved. The data points are then clustered according to the sliding window in which they reside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./1*vyz94J_76dsVToaa4VG1Zg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advantages: fast, linear complexity O(n)  no need to select the number of clusters .\n",
    "\n",
    "disadvantages: \n",
    "1. selection of the window size/radius “r” can be non-trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "\n",
    "DBSCAN is a density based clustered algorithm similar to mean-shift, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./1*tc8UF-h0nQqUfLC8-0uInQ.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DBSCAN begins with an arbitrary starting data point that has not been visited. The neighborhood of this point is extracted using a distance epsilon ε (All points which are within the ε distance are neighborhood points).\n",
    "\n",
    "2. If there are a sufficient number of points (according to minPoints) within this neighborhood then the clustering process starts and the current data point becomes the first point in the new cluster. Otherwise, the point will be labeled as noise (later this noisy point might become the part of the cluster). In both cases that point is marked as “visited”.\n",
    "\n",
    "3. For this first point in the new cluster, the points within its ε distance neighborhood also become part of the same cluster. This procedure of making all points in the ε neighborhood belong to the same cluster is then repeated for all of the new points that have been just added to the cluster group.\n",
    "\n",
    "4. This process of steps 2 and 3 is repeated until all points in the cluster are determined i.e all points within the ε neighborhood of the cluster have been visited and labelled.\n",
    "\n",
    "5. Once we’re done with the current cluster, a new unvisited point is retrieved and processed, leading to the discovery of a further cluster or noise. This process repeats until all points are marked as visited. Since at the end of this all points have been visited, each point well have been marked as either belonging to a cluster or being noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### advantages: \n",
    "1. fast, linear complexity O(n)  no need to select the number of clusters .\n",
    "2.  identifies outliers as noises unlike mean-shift\n",
    "\n",
    "3. able to find arbitrarily sized and arbitrarily shaped clusters well.\n",
    "\n",
    "disadvantages: \n",
    "1. doesn’t perform as well as others when the clusters are of varying density. Since  setting of the distance threshold ε and minPoints for identifying the neighborhood points will vary from cluster to cluster when the density varies.\n",
    "\n",
    "2. finding ε for high-dimensional data is very challenging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Comparing different clustering algorithms on toy datasets\n",
    "\n",
    "\n",
    "This example shows characteristics of different\n",
    "clustering algorithms on datasets that are \"interesting\"\n",
    "but still in 2D. With the exception of the last dataset,\n",
    "the parameters of each of these dataset-algorithm pairs\n",
    "has been tuned to produce good clustering results. Some\n",
    "algorithms are more sensitive to parameter values than\n",
    "others.\n",
    "\n",
    "The last dataset is an example of a 'null' situation for\n",
    "clustering: the data is homogeneous, and there is no good\n",
    "clustering. For this example, the null dataset uses the\n",
    "same parameters as the dataset in the row above it, which\n",
    "represents a mismatch in the parameter values and the\n",
    "data structure.\n",
    "\n",
    "While these examples give some intuition about the\n",
    "algorithms, this intuition might not apply to very high\n",
    "dimensional data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please Restart and Clear output for your kernel before running this second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(__doc__)\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing random seed for equivalent results for everyone\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "\n",
    "\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "\n",
    "\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :4]  # we only take the first two features.\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans, SpectralClustering\n",
    "import numpy as np\n",
    "X = np.array(X)\n",
    "#clustering = DBSCAN(.55).fit(X)\n",
    "#clustering = KMeans(3).fit(X)\n",
    "clustering = SpectralClustering(n_clusters=3).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
